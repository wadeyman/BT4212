{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wadeyman/BT4212/blob/main/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri5vLfKxHq8f"
      },
      "source": [
        "# BT4212 Homework 4\n",
        "## Search Engine Optimization and Analytics\n",
        "\n",
        "Term: Fall 2023\n",
        "\n",
        "*Individual Assignment, due Nov 5，5PM*\n",
        "\n",
        "## Submission Instruction\n",
        "\n",
        "This homework contains several coding tasks and short-answer questions to explore several predictive models for page-rank. For coding part, please write codes in the corresponding cells. I may provide some comment lines as guideline. For short-answers, type your answer in the cells with **ANSWER: HERE**. Please double click those cells and directly input your answer.\n",
        "\n",
        "I recommend you use Python 3 for this homework. Python 2 may not be supported. You can use either your own PC or Google Colab to do this homework. GPU support is NOT required.\n",
        "\n",
        "\n",
        "Save your notebook `.ipynb` file as `StudentID_YourName_HW4.ipynb`. Generate an `.html` file from `.ipynb` file and save as `StudentID_YourName_HW4.html`. Zip your notebook file and html file into a single `.zip` file.\n",
        "\n",
        "Upload the `zip` file as `StudentID_YourName_HW4.zip`. Please DO NOT include data file in your zip file (too large to upload and download).\n",
        "\n",
        "**Please make sure your code is executable.**\n",
        "\n",
        "If you are using Google Colab,\n",
        "* Please make sure you have expanded all hidden cells. You can refer to https://stackoverflow.com/questions/62457417/unhide-all-cells-in-google-colab for more information.\n",
        "* How to generate an HTML file from your notebook file in Google Colab? Please refer to https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab\n",
        "\n",
        "HW4 is worth 80 points in total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "dZjyjb9VHq8h"
      },
      "outputs": [],
      "source": [
        "# Input your name and studentID\n",
        "name = \"Tan Hui Rong\"\n",
        "stuID = \"A0216246J\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "HrBn586bHq8i"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "# the recommended version is listed but you could try using the most updated one.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import statsmodels.api as sm # recommended version: 0.13.0\n",
        "from statsmodels.miscmodels.ordinal_model import OrderedModel\n",
        "\n",
        "import xgboost as xgb # recommended version: 1.5.0\n",
        "from xgboost import plot_importance\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "jUOoBeXfHq8i"
      },
      "outputs": [],
      "source": [
        "# To fix random seeds (Note that you may still get slightly different results.)\n",
        "np.random.seed(12345)\n",
        "\n",
        "# To ignore some warnings\n",
        "import warnings\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "warnings.simplefilter('ignore', ConvergenceWarning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "0rMYA3GuHq8i"
      },
      "outputs": [],
      "source": [
        "## If you are using Google Colab and get an error regarding statsmodels(OrderedModel),\n",
        "## you can use the following line to re-install statsmodels package.\n",
        "## This will takes some time, please RESTART the RUNTIME after installation.\n",
        "\n",
        "# !pip install statsmodels==0.13.0\n",
        "\n",
        "## You may also want to mount your Google Drive to allow easy file loading.\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwgNA1wAIBRk",
        "outputId": "2bf80744-730d-474b-ac58-3bfee0a7db2d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7u31z7fHq8i"
      },
      "source": [
        "## Background Information: About the Data\n",
        "The data set is acquired in the following steps.\n",
        "1. Identify a set of 20 keywords.\n",
        "2. For each keyword, search in google, and return the first 98 websites. It has $20\\times98=1960$ observations in total.\n",
        "3.  Split the dataset into training and test (or validation) data. The training data includes $70\\%$ of observations ($14\\times 98=1372$ rows) while the test one has $30\\%$ of observations ($6\\times 98=588$ rows)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzpLBOI8Hq8j"
      },
      "source": [
        "There are two data files. `Train_dta.csv` for training data and `Test_dta.csv` for testing data. Open the data file with Excel may encounter some unexpected errors. Simply download another copy from Canvas, if it occurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPYBqvj1Hq8j"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "AaZOVI-tHq8j"
      },
      "outputs": [],
      "source": [
        "# Load the training and test datasets and print the first 5 rows of the training dataset.\n",
        "# Please use pd.read_csv(\"data.csv\") to load your data\n",
        "# Run this cell\n",
        "\n",
        "feature_col =[\"TitleFlag\", \"TitleDensity\", \"URLFlag\", \"URLDensity\", \"MetaFlag\", \"MetaDensity\", \"PageAuthority\", \"DomainAuthority\", \"LinkingDomain\", \"InboundLink\" , \"RankingKeyword\" ]\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Train_dta.csv\")\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Test_dta.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "qweLGTR7J8n6",
        "outputId": "06c2c9e6-e5d3-4b67-945e-bd96ee479436"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID  Position  ReverseRank  \\\n",
              "0   1         1          100   \n",
              "1   2         2           99   \n",
              "2   3         3           98   \n",
              "3   4         4           97   \n",
              "4   5         5           96   \n",
              "\n",
              "                                               Title  \\\n",
              "0  Job Interview Questions Singapore: Answers to ...   \n",
              "1         Job Interview Questions - Hudson Singapore   \n",
              "2  20 tips that will help you ace your next job i...   \n",
              "3  Common interview questions and answers | Rober...   \n",
              "4  10 Common Interview Questions and How to ... -...   \n",
              "\n",
              "                                                 URL  \\\n",
              "0  https://blog.carousell.com/job-interview-quest...   \n",
              "1  https://www.hudson.sg/career-advice/job-interv...   \n",
              "2  http://www.asiaone.com/business/20-tips-will-h...   \n",
              "3  https://www.roberthalf.com.sg/career-advice/in...   \n",
              "4  http://gradsingapore.com/graduate-careers-advi...   \n",
              "\n",
              "                                                Meta  TitleFlag  TitleDensity  \\\n",
              "0  sep 19 2017 common job interview questions in ...          0        0.3000   \n",
              "1  discover the five most common job interview qu...          0        0.6000   \n",
              "2  oct 8 2017 with singapore's unemployment rate ...          0        0.2143   \n",
              "3  knowing what job interview questions you might...          0        0.1429   \n",
              "4  don't be caught off guard in an interview try ...          0        0.2000   \n",
              "\n",
              "   URLFlag  URLDensity  MetaFlag  MetaDensity  PageAuthority  DomainAuthority  \\\n",
              "0        0      0.4286         0       0.1017             32               60   \n",
              "1        0      0.2500         0       0.2174             19               30   \n",
              "2        0      0.2143         0       0.0645             40               80   \n",
              "3        0      0.1111         0       0.2500             22               40   \n",
              "4        0      0.0714         0       0.1429             24               33   \n",
              "\n",
              "   LinkingDomain  InboundLink  RankingKeyword  \n",
              "0              4          5.0               4  \n",
              "1              0          0.0               9  \n",
              "2              3        400.0               0  \n",
              "3              0          0.0               4  \n",
              "4              2          2.0               6  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-395d6314-8963-4d0c-8c06-40bb1e1d6393\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Position</th>\n",
              "      <th>ReverseRank</th>\n",
              "      <th>Title</th>\n",
              "      <th>URL</th>\n",
              "      <th>Meta</th>\n",
              "      <th>TitleFlag</th>\n",
              "      <th>TitleDensity</th>\n",
              "      <th>URLFlag</th>\n",
              "      <th>URLDensity</th>\n",
              "      <th>MetaFlag</th>\n",
              "      <th>MetaDensity</th>\n",
              "      <th>PageAuthority</th>\n",
              "      <th>DomainAuthority</th>\n",
              "      <th>LinkingDomain</th>\n",
              "      <th>InboundLink</th>\n",
              "      <th>RankingKeyword</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>100</td>\n",
              "      <td>Job Interview Questions Singapore: Answers to ...</td>\n",
              "      <td>https://blog.carousell.com/job-interview-quest...</td>\n",
              "      <td>sep 19 2017 common job interview questions in ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.3000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.4286</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1017</td>\n",
              "      <td>32</td>\n",
              "      <td>60</td>\n",
              "      <td>4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>99</td>\n",
              "      <td>Job Interview Questions - Hudson Singapore</td>\n",
              "      <td>https://www.hudson.sg/career-advice/job-interv...</td>\n",
              "      <td>discover the five most common job interview qu...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.6000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2174</td>\n",
              "      <td>19</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>98</td>\n",
              "      <td>20 tips that will help you ace your next job i...</td>\n",
              "      <td>http://www.asiaone.com/business/20-tips-will-h...</td>\n",
              "      <td>oct 8 2017 with singapore's unemployment rate ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2143</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2143</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0645</td>\n",
              "      <td>40</td>\n",
              "      <td>80</td>\n",
              "      <td>3</td>\n",
              "      <td>400.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>97</td>\n",
              "      <td>Common interview questions and answers | Rober...</td>\n",
              "      <td>https://www.roberthalf.com.sg/career-advice/in...</td>\n",
              "      <td>knowing what job interview questions you might...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1429</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1111</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2500</td>\n",
              "      <td>22</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>96</td>\n",
              "      <td>10 Common Interview Questions and How to ... -...</td>\n",
              "      <td>http://gradsingapore.com/graduate-careers-advi...</td>\n",
              "      <td>don't be caught off guard in an interview try ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.2000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0714</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1429</td>\n",
              "      <td>24</td>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-395d6314-8963-4d0c-8c06-40bb1e1d6393')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-395d6314-8963-4d0c-8c06-40bb1e1d6393 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-395d6314-8963-4d0c-8c06-40bb1e1d6393');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b84ed87d-056e-431b-ba33-1e6d00765c93\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b84ed87d-056e-431b-ba33-1e6d00765c93')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b84ed87d-056e-431b-ba33-1e6d00765c93 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt9n6LTIHq8j"
      },
      "source": [
        "There are many columns in the data, e.g., title, url and meta desciptions. The detailed information about each column is in the appendix.\n",
        "\n",
        "**We will only use: “TitleFlag”, “TitleDensity”, “URLFlag”, “URLDensity”, “MetaFlag”, “MetaDensity”, “PageAuthority”, “DomainAuthority”, “LinkingDomain”, “InboundLink” and “RankingKeyword” as features, \"ReverseRank\" as label.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "-zYEPrmGHq8j"
      },
      "outputs": [],
      "source": [
        "# Split feature and label\n",
        "# Run this cell\n",
        "\n",
        "train_feature = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Train_dta.csv\",usecols=feature_col)\n",
        "train_label = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Train_dta.csv\",usecols=[\"ReverseRank\"])\n",
        "\n",
        "test_feature = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Test_dta.csv\",usecols=feature_col)\n",
        "test_label = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Test_dta.csv\",usecols=[\"ReverseRank\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "nTx8QY89Hq8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138dae9a-4f05-4eaf-f91a-0c79a6d4a01b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1372, 11) (1372, 1) (588, 11) (588, 1)\n"
          ]
        }
      ],
      "source": [
        "# Run this cell\n",
        "print(train_feature.shape,\n",
        "      train_label.shape,\n",
        "      test_feature.shape,\n",
        "      test_label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "sfSRfz-2Hq8j"
      },
      "outputs": [],
      "source": [
        "# Run this cell\n",
        "# This serves as your data input\n",
        "X_train = train_feature # trainig feature, as a dataframe in pandas\n",
        "X_test = test_feature # test feature, as a df\n",
        "\n",
        "y_train = train_label.values # training label, as a numpy array\n",
        "y_test = test_label.values # test label, as a np array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFoR-2eAHq8k"
      },
      "source": [
        "To avoid potential issues of shallow copy and deep copy. Try to load data separately for each problem, although they may be the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGEF-ARrHq8k"
      },
      "source": [
        "## Problem 1 Pointwise Rank. (35 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrLRHJzJHq8k"
      },
      "source": [
        "###  Q1. Linear Regression (10 points)\n",
        "Please use the training dataset to fit a linear regression with all variables aforementioned. Please use the trained linear regression model to predict the rank in the test dataset and **report the root mean square error (RMSE) of the prediction**, i.e., $\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$, where $n$ is the total number of data points in the test dataset, $y_i$ is the true value of the outcome variable in the test dataset, and $\\hat{y}_i$ is the model prediction of the outcome variable. (5 points)\n",
        "\n",
        "Reference: https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "O8ra_dPtHq8k"
      },
      "outputs": [],
      "source": [
        "# Data Input\n",
        "X_train = train_feature # trainig feature, as a dataframe in pandas\n",
        "X_test = test_feature # test feature, as a df\n",
        "\n",
        "y_train = train_label.values # training label, as a numpy array\n",
        "y_test = test_label.values # test label, as a np array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ef6_HrNfHq8k"
      },
      "outputs": [],
      "source": [
        "# Model: USE sm.OLS(y_train,X_train)\n",
        "# Fit: USE model.fit()\n",
        "# y_train is a numpy array from train_label, X_train is a pandas dataframe from train_feature.\n",
        "# Remember to add an intercept by sm.add_constant() to both train and test data!\n",
        "X_train = sm.add_constant(X_train)\n",
        "X_test = sm.add_constant(X_test)\n",
        "lin_model = sm.OLS(y_train, X_train)\n",
        "results = lin_model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "mGf5ApwGHq8k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "eb208bf2-6522-42d1-9122-c472be816b9e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                            OLS Regression Results                            \n",
              "==============================================================================\n",
              "Dep. Variable:                      y   R-squared:                       0.104\n",
              "Model:                            OLS   Adj. R-squared:                  0.097\n",
              "Method:                 Least Squares   F-statistic:                     14.39\n",
              "Date:                Thu, 26 Oct 2023   Prob (F-statistic):           1.33e-26\n",
              "Time:                        17:33:42   Log-Likelihood:                -6457.1\n",
              "No. Observations:                1372   AIC:                         1.294e+04\n",
              "Df Residuals:                    1360   BIC:                         1.300e+04\n",
              "Df Model:                          11                                         \n",
              "Covariance Type:            nonrobust                                         \n",
              "===================================================================================\n",
              "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
              "-----------------------------------------------------------------------------------\n",
              "const              32.4841      3.072     10.575      0.000      26.458      38.510\n",
              "TitleFlag          11.0206      4.107      2.683      0.007       2.964      19.078\n",
              "TitleDensity       15.7273      5.382      2.922      0.004       5.168      26.286\n",
              "URLFlag            -2.5156      4.326     -0.581      0.561     -11.003       5.972\n",
              "URLDensity         34.8241      7.117      4.893      0.000      20.862      48.786\n",
              "MetaFlag           -3.3674      2.005     -1.680      0.093      -7.300       0.565\n",
              "MetaDensity        18.6692     11.354      1.644      0.100      -3.605      40.943\n",
              "PageAuthority       0.0907      0.123      0.737      0.461      -0.151       0.332\n",
              "DomainAuthority     0.1167      0.055      2.122      0.034       0.009       0.225\n",
              "LinkingDomain      -0.0012      0.001     -1.342      0.180      -0.003       0.001\n",
              "InboundLink     -1.855e-05   2.88e-05     -0.645      0.519    -7.5e-05    3.79e-05\n",
              "RankingKeyword     -0.0043      0.003     -1.342      0.180      -0.011       0.002\n",
              "==============================================================================\n",
              "Omnibus:                      294.603   Durbin-Watson:                   0.264\n",
              "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               58.136\n",
              "Skew:                          -0.077   Prob(JB):                     2.38e-13\n",
              "Kurtosis:                       2.004   Cond. No.                     7.44e+05\n",
              "==============================================================================\n",
              "\n",
              "Notes:\n",
              "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
              "[2] The condition number is large, 7.44e+05. This might indicate that there are\n",
              "strong multicollinearity or other numerical problems.\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OLS Regression Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.104</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.097</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   14.39</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>             <td>Thu, 26 Oct 2023</td> <th>  Prob (F-statistic):</th> <td>1.33e-26</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                 <td>17:33:42</td>     <th>  Log-Likelihood:    </th> <td> -6457.1</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>      <td>  1372</td>      <th>  AIC:               </th> <td>1.294e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>          <td>  1360</td>      <th>  BIC:               </th> <td>1.300e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>const</th>           <td>   32.4841</td> <td>    3.072</td> <td>   10.575</td> <td> 0.000</td> <td>   26.458</td> <td>   38.510</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>TitleFlag</th>       <td>   11.0206</td> <td>    4.107</td> <td>    2.683</td> <td> 0.007</td> <td>    2.964</td> <td>   19.078</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>TitleDensity</th>    <td>   15.7273</td> <td>    5.382</td> <td>    2.922</td> <td> 0.004</td> <td>    5.168</td> <td>   26.286</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>URLFlag</th>         <td>   -2.5156</td> <td>    4.326</td> <td>   -0.581</td> <td> 0.561</td> <td>  -11.003</td> <td>    5.972</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>URLDensity</th>      <td>   34.8241</td> <td>    7.117</td> <td>    4.893</td> <td> 0.000</td> <td>   20.862</td> <td>   48.786</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>MetaFlag</th>        <td>   -3.3674</td> <td>    2.005</td> <td>   -1.680</td> <td> 0.093</td> <td>   -7.300</td> <td>    0.565</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>MetaDensity</th>     <td>   18.6692</td> <td>   11.354</td> <td>    1.644</td> <td> 0.100</td> <td>   -3.605</td> <td>   40.943</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>PageAuthority</th>   <td>    0.0907</td> <td>    0.123</td> <td>    0.737</td> <td> 0.461</td> <td>   -0.151</td> <td>    0.332</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>DomainAuthority</th> <td>    0.1167</td> <td>    0.055</td> <td>    2.122</td> <td> 0.034</td> <td>    0.009</td> <td>    0.225</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>LinkingDomain</th>   <td>   -0.0012</td> <td>    0.001</td> <td>   -1.342</td> <td> 0.180</td> <td>   -0.003</td> <td>    0.001</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>InboundLink</th>     <td>-1.855e-05</td> <td> 2.88e-05</td> <td>   -0.645</td> <td> 0.519</td> <td> -7.5e-05</td> <td> 3.79e-05</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>RankingKeyword</th>  <td>   -0.0043</td> <td>    0.003</td> <td>   -1.342</td> <td> 0.180</td> <td>   -0.011</td> <td>    0.002</td>\n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Omnibus:</th>       <td>294.603</td> <th>  Durbin-Watson:     </th> <td>   0.264</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>  58.136</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Skew:</th>          <td>-0.077</td>  <th>  Prob(JB):          </th> <td>2.38e-13</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Kurtosis:</th>      <td> 2.004</td>  <th>  Cond. No.          </th> <td>7.44e+05</td>\n",
              "</tr>\n",
              "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 7.44e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &        y         & \\textbf{  R-squared:         } &     0.104   \\\\\n\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.097   \\\\\n\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     14.39   \\\\\n\\textbf{Date:}             & Thu, 26 Oct 2023 & \\textbf{  Prob (F-statistic):} &  1.33e-26   \\\\\n\\textbf{Time:}             &     17:33:42     & \\textbf{  Log-Likelihood:    } &   -6457.1   \\\\\n\\textbf{No. Observations:} &        1372      & \\textbf{  AIC:               } & 1.294e+04   \\\\\n\\textbf{Df Residuals:}     &        1360      & \\textbf{  BIC:               } & 1.300e+04   \\\\\n\\textbf{Df Model:}         &          11      & \\textbf{                     } &             \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{const}           &      32.4841  &        3.072     &    10.575  &         0.000        &       26.458    &       38.510     \\\\\n\\textbf{TitleFlag}       &      11.0206  &        4.107     &     2.683  &         0.007        &        2.964    &       19.078     \\\\\n\\textbf{TitleDensity}    &      15.7273  &        5.382     &     2.922  &         0.004        &        5.168    &       26.286     \\\\\n\\textbf{URLFlag}         &      -2.5156  &        4.326     &    -0.581  &         0.561        &      -11.003    &        5.972     \\\\\n\\textbf{URLDensity}      &      34.8241  &        7.117     &     4.893  &         0.000        &       20.862    &       48.786     \\\\\n\\textbf{MetaFlag}        &      -3.3674  &        2.005     &    -1.680  &         0.093        &       -7.300    &        0.565     \\\\\n\\textbf{MetaDensity}     &      18.6692  &       11.354     &     1.644  &         0.100        &       -3.605    &       40.943     \\\\\n\\textbf{PageAuthority}   &       0.0907  &        0.123     &     0.737  &         0.461        &       -0.151    &        0.332     \\\\\n\\textbf{DomainAuthority} &       0.1167  &        0.055     &     2.122  &         0.034        &        0.009    &        0.225     \\\\\n\\textbf{LinkingDomain}   &      -0.0012  &        0.001     &    -1.342  &         0.180        &       -0.003    &        0.001     \\\\\n\\textbf{InboundLink}     &   -1.855e-05  &     2.88e-05     &    -0.645  &         0.519        &     -7.5e-05    &     3.79e-05     \\\\\n\\textbf{RankingKeyword}  &      -0.0043  &        0.003     &    -1.342  &         0.180        &       -0.011    &        0.002     \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lclc}\n\\textbf{Omnibus:}       & 294.603 & \\textbf{  Durbin-Watson:     } &    0.264  \\\\\n\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &   58.136  \\\\\n\\textbf{Skew:}          &  -0.077 & \\textbf{  Prob(JB):          } & 2.38e-13  \\\\\n\\textbf{Kurtosis:}      &   2.004 & \\textbf{  Cond. No.          } & 7.44e+05  \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OLS Regression Results}\n\\end{center}\n\nNotes: \\newline\n [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n [2] The condition number is large, 7.44e+05. This might indicate that there are \\newline\n strong multicollinearity or other numerical problems."
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "# Print model summary by model.summary()\n",
        "results.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "rdLzHgXrHq8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b42fb40c-c1c3-4909-ec02-0fbb2c8135d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      53.664124\n",
              "1      56.095566\n",
              "2      66.037076\n",
              "3      65.902540\n",
              "4      52.730487\n",
              "         ...    \n",
              "583    42.524573\n",
              "584   -18.311877\n",
              "585    37.341913\n",
              "586    65.034483\n",
              "587    37.293170\n",
              "Length: 588, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# Make prediction\n",
        "predictions = results.predict(X_test)\n",
        "predictions[:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = pd.Series(pd.DataFrame(y_test)[0])"
      ],
      "metadata": {
        "id": "hm-iMW8OAfs3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm0T1WR3_Kwu",
        "outputId": "66b0a116-4bb7-43f7-e5ce-60adcee77784"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "U2zheO6tHq8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2424dc2c-ec08-4036-8e21-8453275f8889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 37.150724570848375\n"
          ]
        }
      ],
      "source": [
        "# Print RMSE, y_test is a numpy array from test_label, y_pred is a numpy array from your model prediction\n",
        "resids = y_test - predictions\n",
        "mse = np.mean(resids**2)\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"RMSE:\", rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hmBjhgOHq8l"
      },
      "source": [
        "**Pick the most statistically significant variable and interpret its estimated beta coefficient.** (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LztAvKiHq8l"
      },
      "source": [
        "*ANSWER: HERE*\n",
        "\n",
        "URL density gives a P-value closest to 0, suggesting it is the msot statistically significant variable. The estimated beta coefficient is 34.8241. This means for every 1 unit increase in URL density, we see a positive change in dependent variable ReverseRank, holding all other variables constant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPilo6QPHq8l"
      },
      "source": [
        "### Q2. Logistic Regression (15 points)\n",
        "\n",
        "For the baseline logistic regression, it does not require an ordinal relationship among the levels of the outcome variable, e.g., level 1 does not necessarily imply superiority or inferiority compared with level 2. However, for ordinal variable, its levels can be ranked implying a higher value than other level, e.g., in the school grade, A is better than B. We will use **ordinal** logistic regression for this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-i3QjueHq8l"
      },
      "source": [
        "Please use the training dataset to fit an **ordinal** logistic regression model with all variables aforementioned. **Use the `ReverseRank` as the outcome variable**, that is apparently a ranked variable**. (5 points)\n",
        "\n",
        "Note that you may not be able to run ordinal logistic regression because the distributions of several variables are too skewed. You can use `new_variable = np.log(the_problematic_variable+1)` to **transform those variables in both training and testing data.** (5 points)\n",
        "\n",
        "Please use the trained ordinal logistic regression model to predict the rank in the test dataset. **Print the predicted rank on the test dataset and report the RMSE of the prediction.** （5 points）\n",
        "\n",
        "Reference: https://www.statsmodels.org/dev/examples/notebooks/generated/ordinal_regression.html.\n",
        "https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/.\n",
        "\n",
        "Remarks: You will find tons of useful materials about statistical modelling in UCLA website, even though mostly implemented with R or Stata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "MujY5DHeHq8l"
      },
      "outputs": [],
      "source": [
        "# Data Input\n",
        "X_train = train_feature # trainig feature, as a dataframe in pandas\n",
        "X_test = test_feature # test feature, as a df\n",
        "\n",
        "y_train = train_label.values # training label, as a numpy array\n",
        "y_test = test_label.values # test label, as a np array"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables with high skewness values (highly positive or negative) may benefit from log transformation to reduce the skewness. Highly skewed variables can affect the performance and interpretability of some statistical models. In this case, linear models assume homoskedastic being that the residuals (the differences between the observed and predicted values) are normally distributed and have constant variance. Skewed data violates this assumption, leading to unreliability.\n",
        "\n",
        "TitleFlag, URLFlag, LinkingDomain, InboundLink, RankingKeyword variables have skewness values above 1 indicating significant positive skewness. Shall leave MetaFlag and MetaDensity as they are moderately skewed and we want to retain as much of the original data."
      ],
      "metadata": {
        "id": "v3jVnEwdCXy-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "DGquYtkrHq8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d11a4d-332b-4b77-e14e-3e0c82a9be5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TitleFlag           4.018238\n",
              "TitleDensity        0.755273\n",
              "URLFlag             4.343633\n",
              "URLDensity          0.872202\n",
              "MetaFlag            1.733323\n",
              "MetaDensity         1.122730\n",
              "PageAuthority       0.362187\n",
              "DomainAuthority     0.187663\n",
              "LinkingDomain      15.575515\n",
              "InboundLink        16.606262\n",
              "RankingKeyword     12.241945\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# Adjust your input\n",
        "# Please specify which parts of data you do a log transformation on.\n",
        "# There are many ways to determine the skewness,\n",
        "# e.g., plotting the distribution, calling pandas.DataFrame.skew, etc.\n",
        "# As long as you provide reasons for the transformation, you will be awarded the points.\n",
        "pd.DataFrame.skew(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_log = [\"TitleFlag\", \"URLFlag\", \"LinkingDomain\", \"InboundLink\", \"RankingKeyword\"]\n",
        "X_train_withLogs = X_train\n",
        "X_train_withLogs[columns_to_log] = X_train_withLogs[columns_to_log].apply(lambda x: np.log(x + 1))"
      ],
      "metadata": {
        "id": "FilzSAK_D-Sk"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "K8oZeqhWHq8m"
      },
      "outputs": [],
      "source": [
        "# Model: use OrderedModel(y_train, X_train, distr='logit')\n",
        "# Fit: use model.fit(method='bfgs', disp=False)\n",
        "# y_train is a numpy array from train_label, X_train is a pandas dataframe from train_feature.\n",
        "ord_model = OrderedModel(y_train, X_train_withLogs)\n",
        "results = ord_model.fit(method='bfgs', disp=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "93wLi0VcHq8m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "adc14c90-f359-4ac1-e55a-cafd189d4bce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.summary.Summary'>\n",
              "\"\"\"\n",
              "                             OrderedModel Results                             \n",
              "==============================================================================\n",
              "Dep. Variable:                      y   Log-Likelihood:                -6220.9\n",
              "Model:                   OrderedModel   AIC:                         1.266e+04\n",
              "Method:            Maximum Likelihood   BIC:                         1.322e+04\n",
              "Date:                Thu, 26 Oct 2023                                         \n",
              "Time:                        17:34:35                                         \n",
              "No. Observations:                1372                                         \n",
              "Df Residuals:                    1264                                         \n",
              "Df Model:                          11                                         \n",
              "===================================================================================\n",
              "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
              "-----------------------------------------------------------------------------------\n",
              "TitleFlag           0.6330      0.223      2.842      0.004       0.196       1.070\n",
              "TitleDensity        0.5903      0.204      2.895      0.004       0.191       0.990\n",
              "URLFlag            -0.0890      0.234     -0.381      0.703      -0.547       0.369\n",
              "URLDensity          1.3060      0.269      4.854      0.000       0.779       1.833\n",
              "MetaFlag           -0.1294      0.075     -1.730      0.084      -0.276       0.017\n",
              "MetaDensity         0.8170      0.423      1.929      0.054      -0.013       1.647\n",
              "PageAuthority       0.0016      0.012      0.137      0.891      -0.022       0.025\n",
              "DomainAuthority     0.0037      0.005      0.791      0.429      -0.005       0.013\n",
              "LinkingDomain      -0.0133      0.061     -0.218      0.827      -0.133       0.106\n",
              "InboundLink        -0.0191      0.031     -0.622      0.534      -0.079       0.041\n",
              "RankingKeyword      0.0414      0.020      2.028      0.043       0.001       0.081\n",
              "3/4                -1.7461      0.161    -10.845      0.000      -2.062      -1.431\n",
              "4/5                -1.2722      0.261     -4.872      0.000      -1.784      -0.760\n",
              "5/6                -1.7326      0.262     -6.622      0.000      -2.245      -1.220\n",
              "6/7                -2.0233      0.262     -7.709      0.000      -2.538      -1.509\n",
              "7/8                -2.2335      0.263     -8.490      0.000      -2.749      -1.718\n",
              "8/9                -2.3962      0.264     -9.093      0.000      -2.913      -1.880\n",
              "9/10               -2.5229      0.264     -9.563      0.000      -3.040      -2.006\n",
              "10/11              -2.6232      0.264     -9.935      0.000      -3.141      -2.106\n",
              "11/12              -2.7155      0.264    -10.275      0.000      -3.233      -2.198\n",
              "12/13              -2.8004      0.264    -10.589      0.000      -3.319      -2.282\n",
              "13/14              -2.8741      0.265    -10.860      0.000      -3.393      -2.355\n",
              "14/15              -2.9390      0.265    -11.100      0.000      -3.458      -2.420\n",
              "15/16              -2.9934      0.265    -11.301      0.000      -3.513      -2.474\n",
              "16/17              -3.0443      0.265    -11.489      0.000      -3.564      -2.525\n",
              "17/18              -3.0910      0.265    -11.662      0.000      -3.610      -2.571\n",
              "18/19              -3.1337      0.265    -11.819      0.000      -3.653      -2.614\n",
              "19/20              -3.1773      0.265    -11.980      0.000      -3.697      -2.657\n",
              "20/21              -3.2160      0.265    -12.123      0.000      -3.736      -2.696\n",
              "21/22              -3.2487      0.265    -12.244      0.000      -3.769      -2.729\n",
              "22/23              -3.2792      0.265    -12.357      0.000      -3.799      -2.759\n",
              "23/24              -3.3089      0.265    -12.466      0.000      -3.829      -2.789\n",
              "24/25              -3.3370      0.265    -12.570      0.000      -3.857      -2.817\n",
              "25/26              -3.3610      0.266    -12.659      0.000      -3.881      -2.841\n",
              "26/27              -3.3824      0.266    -12.738      0.000      -3.903      -2.862\n",
              "27/28              -3.4034      0.266    -12.816      0.000      -3.924      -2.883\n",
              "28/29              -3.4225      0.266    -12.886      0.000      -3.943      -2.902\n",
              "29/30              -3.4382      0.266    -12.945      0.000      -3.959      -2.918\n",
              "30/31              -3.4532      0.266    -13.000      0.000      -3.974      -2.933\n",
              "31/32              -3.4707      0.266    -13.065      0.000      -3.991      -2.950\n",
              "32/33              -3.4870      0.266    -13.125      0.000      -4.008      -2.966\n",
              "33/34              -3.4993      0.266    -13.171      0.000      -4.020      -2.979\n",
              "34/35              -3.5110      0.266    -13.214      0.000      -4.032      -2.990\n",
              "35/36              -3.5221      0.266    -13.255      0.000      -4.043      -3.001\n",
              "36/37              -3.5320      0.266    -13.291      0.000      -4.053      -3.011\n",
              "37/38              -3.5415      0.266    -13.327      0.000      -4.062      -3.021\n",
              "38/39              -3.5526      0.266    -13.368      0.000      -4.073      -3.032\n",
              "39/40              -3.5623      0.266    -13.404      0.000      -4.083      -3.041\n",
              "40/41              -3.5696      0.266    -13.431      0.000      -4.091      -3.049\n",
              "41/42              -3.5770      0.266    -13.458      0.000      -4.098      -3.056\n",
              "42/43              -3.5842      0.266    -13.485      0.000      -4.105      -3.063\n",
              "43/44              -3.5910      0.266    -13.510      0.000      -4.112      -3.070\n",
              "44/45              -3.5941      0.266    -13.521      0.000      -4.115      -3.073\n",
              "45/46              -3.5959      0.266    -13.528      0.000      -4.117      -3.075\n",
              "46/47              -3.5978      0.266    -13.534      0.000      -4.119      -3.077\n",
              "47/48              -3.6007      0.266    -13.545      0.000      -4.122      -3.080\n",
              "48/49              -3.6064      0.266    -13.567      0.000      -4.127      -3.085\n",
              "49/50              -3.6091      0.266    -13.576      0.000      -4.130      -3.088\n",
              "50/51              -3.6087      0.266    -13.575      0.000      -4.130      -3.088\n",
              "51/52              -3.6107      0.266    -13.582      0.000      -4.132      -3.090\n",
              "52/53              -3.6131      0.266    -13.591      0.000      -4.134      -3.092\n",
              "53/54              -3.6148      0.266    -13.597      0.000      -4.136      -3.094\n",
              "54/55              -3.6149      0.266    -13.598      0.000      -4.136      -3.094\n",
              "55/56              -3.6140      0.266    -13.594      0.000      -4.135      -3.093\n",
              "56/57              -3.6124      0.266    -13.588      0.000      -4.133      -3.091\n",
              "57/58              -3.6087      0.266    -13.574      0.000      -4.130      -3.088\n",
              "58/59              -3.6032      0.266    -13.554      0.000      -4.124      -3.082\n",
              "59/60              -3.5969      0.266    -13.531      0.000      -4.118      -3.076\n",
              "60/61              -3.5896      0.266    -13.504      0.000      -4.111      -3.069\n",
              "61/62              -3.5816      0.266    -13.474      0.000      -4.103      -3.061\n",
              "62/63              -3.5731      0.266    -13.443      0.000      -4.094      -3.052\n",
              "63/64              -3.5643      0.266    -13.410      0.000      -4.085      -3.043\n",
              "64/65              -3.5569      0.266    -13.383      0.000      -4.078      -3.036\n",
              "65/66              -3.5471      0.266    -13.347      0.000      -4.068      -3.026\n",
              "66/67              -3.5352      0.266    -13.303      0.000      -4.056      -3.014\n",
              "67/68              -3.5252      0.266    -13.266      0.000      -4.046      -3.004\n",
              "68/69              -3.5136      0.266    -13.223      0.000      -4.034      -2.993\n",
              "69/70              -3.5010      0.266    -13.176      0.000      -4.022      -2.980\n",
              "70/71              -3.4858      0.266    -13.120      0.000      -4.007      -2.965\n",
              "71/72              -3.4664      0.266    -13.048      0.000      -3.987      -2.946\n",
              "72/73              -3.4462      0.266    -12.974      0.000      -3.967      -2.926\n",
              "73/74              -3.4269      0.266    -12.902      0.000      -3.948      -2.906\n",
              "74/75              -3.4083      0.266    -12.833      0.000      -3.929      -2.888\n",
              "75/76              -3.3895      0.266    -12.764      0.000      -3.910      -2.869\n",
              "76/77              -3.3674      0.266    -12.682      0.000      -3.888      -2.847\n",
              "77/78              -3.3423      0.265    -12.589      0.000      -3.863      -2.822\n",
              "78/79              -3.3185      0.265    -12.501      0.000      -3.839      -2.798\n",
              "79/80              -3.2908      0.265    -12.399      0.000      -3.811      -2.771\n",
              "80/81              -3.2592      0.265    -12.282      0.000      -3.779      -2.739\n",
              "81/82              -3.2287      0.265    -12.169      0.000      -3.749      -2.709\n",
              "82/83              -3.1990      0.265    -12.060      0.000      -3.719      -2.679\n",
              "83/84              -3.1640      0.265    -11.930      0.000      -3.684      -2.644\n",
              "84/85              -3.1219      0.265    -11.775      0.000      -3.642      -2.602\n",
              "85/86              -3.0835      0.265    -11.634      0.000      -3.603      -2.564\n",
              "86/87              -3.0442      0.265    -11.488      0.000      -3.564      -2.525\n",
              "87/88              -2.9948      0.265    -11.306      0.000      -3.514      -2.476\n",
              "88/89              -2.9406      0.265    -11.106      0.000      -3.460      -2.422\n",
              "89/90              -2.8827      0.265    -10.892      0.000      -3.401      -2.364\n",
              "90/91              -2.8194      0.265    -10.659      0.000      -3.338      -2.301\n",
              "91/92              -2.7536      0.264    -10.416      0.000      -3.272      -2.235\n",
              "92/93              -2.6749      0.264    -10.125      0.000      -3.193      -2.157\n",
              "93/94              -2.5761      0.264     -9.760      0.000      -3.093      -2.059\n",
              "94/95              -2.4602      0.264     -9.333      0.000      -2.977      -1.944\n",
              "95/96              -2.3271      0.263     -8.842      0.000      -2.843      -1.811\n",
              "96/97              -2.1707      0.263     -8.263      0.000      -2.686      -1.656\n",
              "97/98              -1.9721      0.262     -7.526      0.000      -2.486      -1.459\n",
              "98/99              -1.6922      0.261     -6.478      0.000      -2.204      -1.180\n",
              "99/100             -1.2341      0.261     -4.734      0.000      -1.745      -0.723\n",
              "===================================================================================\n",
              "\"\"\""
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<caption>OrderedModel Results</caption>\n",
              "<tr>\n",
              "  <th>Dep. Variable:</th>             <td>y</td>         <th>  Log-Likelihood:    </th> <td> -6220.9</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Model:</th>               <td>OrderedModel</td>    <th>  AIC:               </th> <td>1.266e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Method:</th>           <td>Maximum Likelihood</td> <th>  BIC:               </th> <td>1.322e+04</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Date:</th>              <td>Thu, 26 Oct 2023</td>  <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Time:</th>                  <td>17:34:35</td>      <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>No. Observations:</th>       <td>  1372</td>       <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Residuals:</th>           <td>  1264</td>       <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Df Model:</th>               <td>    11</td>       <th>                     </th>     <td> </td>    \n",
              "</tr>\n",
              "</table>\n",
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "         <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>TitleFlag</th>       <td>    0.6330</td> <td>    0.223</td> <td>    2.842</td> <td> 0.004</td> <td>    0.196</td> <td>    1.070</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>TitleDensity</th>    <td>    0.5903</td> <td>    0.204</td> <td>    2.895</td> <td> 0.004</td> <td>    0.191</td> <td>    0.990</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>URLFlag</th>         <td>   -0.0890</td> <td>    0.234</td> <td>   -0.381</td> <td> 0.703</td> <td>   -0.547</td> <td>    0.369</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>URLDensity</th>      <td>    1.3060</td> <td>    0.269</td> <td>    4.854</td> <td> 0.000</td> <td>    0.779</td> <td>    1.833</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>MetaFlag</th>        <td>   -0.1294</td> <td>    0.075</td> <td>   -1.730</td> <td> 0.084</td> <td>   -0.276</td> <td>    0.017</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>MetaDensity</th>     <td>    0.8170</td> <td>    0.423</td> <td>    1.929</td> <td> 0.054</td> <td>   -0.013</td> <td>    1.647</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>PageAuthority</th>   <td>    0.0016</td> <td>    0.012</td> <td>    0.137</td> <td> 0.891</td> <td>   -0.022</td> <td>    0.025</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>DomainAuthority</th> <td>    0.0037</td> <td>    0.005</td> <td>    0.791</td> <td> 0.429</td> <td>   -0.005</td> <td>    0.013</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>LinkingDomain</th>   <td>   -0.0133</td> <td>    0.061</td> <td>   -0.218</td> <td> 0.827</td> <td>   -0.133</td> <td>    0.106</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>InboundLink</th>     <td>   -0.0191</td> <td>    0.031</td> <td>   -0.622</td> <td> 0.534</td> <td>   -0.079</td> <td>    0.041</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>RankingKeyword</th>  <td>    0.0414</td> <td>    0.020</td> <td>    2.028</td> <td> 0.043</td> <td>    0.001</td> <td>    0.081</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>3/4</th>             <td>   -1.7461</td> <td>    0.161</td> <td>  -10.845</td> <td> 0.000</td> <td>   -2.062</td> <td>   -1.431</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>4/5</th>             <td>   -1.2722</td> <td>    0.261</td> <td>   -4.872</td> <td> 0.000</td> <td>   -1.784</td> <td>   -0.760</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>5/6</th>             <td>   -1.7326</td> <td>    0.262</td> <td>   -6.622</td> <td> 0.000</td> <td>   -2.245</td> <td>   -1.220</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>6/7</th>             <td>   -2.0233</td> <td>    0.262</td> <td>   -7.709</td> <td> 0.000</td> <td>   -2.538</td> <td>   -1.509</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>7/8</th>             <td>   -2.2335</td> <td>    0.263</td> <td>   -8.490</td> <td> 0.000</td> <td>   -2.749</td> <td>   -1.718</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>8/9</th>             <td>   -2.3962</td> <td>    0.264</td> <td>   -9.093</td> <td> 0.000</td> <td>   -2.913</td> <td>   -1.880</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>9/10</th>            <td>   -2.5229</td> <td>    0.264</td> <td>   -9.563</td> <td> 0.000</td> <td>   -3.040</td> <td>   -2.006</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>10/11</th>           <td>   -2.6232</td> <td>    0.264</td> <td>   -9.935</td> <td> 0.000</td> <td>   -3.141</td> <td>   -2.106</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>11/12</th>           <td>   -2.7155</td> <td>    0.264</td> <td>  -10.275</td> <td> 0.000</td> <td>   -3.233</td> <td>   -2.198</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>12/13</th>           <td>   -2.8004</td> <td>    0.264</td> <td>  -10.589</td> <td> 0.000</td> <td>   -3.319</td> <td>   -2.282</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>13/14</th>           <td>   -2.8741</td> <td>    0.265</td> <td>  -10.860</td> <td> 0.000</td> <td>   -3.393</td> <td>   -2.355</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>14/15</th>           <td>   -2.9390</td> <td>    0.265</td> <td>  -11.100</td> <td> 0.000</td> <td>   -3.458</td> <td>   -2.420</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>15/16</th>           <td>   -2.9934</td> <td>    0.265</td> <td>  -11.301</td> <td> 0.000</td> <td>   -3.513</td> <td>   -2.474</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>16/17</th>           <td>   -3.0443</td> <td>    0.265</td> <td>  -11.489</td> <td> 0.000</td> <td>   -3.564</td> <td>   -2.525</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>17/18</th>           <td>   -3.0910</td> <td>    0.265</td> <td>  -11.662</td> <td> 0.000</td> <td>   -3.610</td> <td>   -2.571</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>18/19</th>           <td>   -3.1337</td> <td>    0.265</td> <td>  -11.819</td> <td> 0.000</td> <td>   -3.653</td> <td>   -2.614</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>19/20</th>           <td>   -3.1773</td> <td>    0.265</td> <td>  -11.980</td> <td> 0.000</td> <td>   -3.697</td> <td>   -2.657</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>20/21</th>           <td>   -3.2160</td> <td>    0.265</td> <td>  -12.123</td> <td> 0.000</td> <td>   -3.736</td> <td>   -2.696</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>21/22</th>           <td>   -3.2487</td> <td>    0.265</td> <td>  -12.244</td> <td> 0.000</td> <td>   -3.769</td> <td>   -2.729</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>22/23</th>           <td>   -3.2792</td> <td>    0.265</td> <td>  -12.357</td> <td> 0.000</td> <td>   -3.799</td> <td>   -2.759</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>23/24</th>           <td>   -3.3089</td> <td>    0.265</td> <td>  -12.466</td> <td> 0.000</td> <td>   -3.829</td> <td>   -2.789</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>24/25</th>           <td>   -3.3370</td> <td>    0.265</td> <td>  -12.570</td> <td> 0.000</td> <td>   -3.857</td> <td>   -2.817</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>25/26</th>           <td>   -3.3610</td> <td>    0.266</td> <td>  -12.659</td> <td> 0.000</td> <td>   -3.881</td> <td>   -2.841</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>26/27</th>           <td>   -3.3824</td> <td>    0.266</td> <td>  -12.738</td> <td> 0.000</td> <td>   -3.903</td> <td>   -2.862</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>27/28</th>           <td>   -3.4034</td> <td>    0.266</td> <td>  -12.816</td> <td> 0.000</td> <td>   -3.924</td> <td>   -2.883</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>28/29</th>           <td>   -3.4225</td> <td>    0.266</td> <td>  -12.886</td> <td> 0.000</td> <td>   -3.943</td> <td>   -2.902</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>29/30</th>           <td>   -3.4382</td> <td>    0.266</td> <td>  -12.945</td> <td> 0.000</td> <td>   -3.959</td> <td>   -2.918</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>30/31</th>           <td>   -3.4532</td> <td>    0.266</td> <td>  -13.000</td> <td> 0.000</td> <td>   -3.974</td> <td>   -2.933</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>31/32</th>           <td>   -3.4707</td> <td>    0.266</td> <td>  -13.065</td> <td> 0.000</td> <td>   -3.991</td> <td>   -2.950</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>32/33</th>           <td>   -3.4870</td> <td>    0.266</td> <td>  -13.125</td> <td> 0.000</td> <td>   -4.008</td> <td>   -2.966</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>33/34</th>           <td>   -3.4993</td> <td>    0.266</td> <td>  -13.171</td> <td> 0.000</td> <td>   -4.020</td> <td>   -2.979</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>34/35</th>           <td>   -3.5110</td> <td>    0.266</td> <td>  -13.214</td> <td> 0.000</td> <td>   -4.032</td> <td>   -2.990</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>35/36</th>           <td>   -3.5221</td> <td>    0.266</td> <td>  -13.255</td> <td> 0.000</td> <td>   -4.043</td> <td>   -3.001</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>36/37</th>           <td>   -3.5320</td> <td>    0.266</td> <td>  -13.291</td> <td> 0.000</td> <td>   -4.053</td> <td>   -3.011</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>37/38</th>           <td>   -3.5415</td> <td>    0.266</td> <td>  -13.327</td> <td> 0.000</td> <td>   -4.062</td> <td>   -3.021</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>38/39</th>           <td>   -3.5526</td> <td>    0.266</td> <td>  -13.368</td> <td> 0.000</td> <td>   -4.073</td> <td>   -3.032</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>39/40</th>           <td>   -3.5623</td> <td>    0.266</td> <td>  -13.404</td> <td> 0.000</td> <td>   -4.083</td> <td>   -3.041</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>40/41</th>           <td>   -3.5696</td> <td>    0.266</td> <td>  -13.431</td> <td> 0.000</td> <td>   -4.091</td> <td>   -3.049</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>41/42</th>           <td>   -3.5770</td> <td>    0.266</td> <td>  -13.458</td> <td> 0.000</td> <td>   -4.098</td> <td>   -3.056</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>42/43</th>           <td>   -3.5842</td> <td>    0.266</td> <td>  -13.485</td> <td> 0.000</td> <td>   -4.105</td> <td>   -3.063</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>43/44</th>           <td>   -3.5910</td> <td>    0.266</td> <td>  -13.510</td> <td> 0.000</td> <td>   -4.112</td> <td>   -3.070</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>44/45</th>           <td>   -3.5941</td> <td>    0.266</td> <td>  -13.521</td> <td> 0.000</td> <td>   -4.115</td> <td>   -3.073</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>45/46</th>           <td>   -3.5959</td> <td>    0.266</td> <td>  -13.528</td> <td> 0.000</td> <td>   -4.117</td> <td>   -3.075</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>46/47</th>           <td>   -3.5978</td> <td>    0.266</td> <td>  -13.534</td> <td> 0.000</td> <td>   -4.119</td> <td>   -3.077</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>47/48</th>           <td>   -3.6007</td> <td>    0.266</td> <td>  -13.545</td> <td> 0.000</td> <td>   -4.122</td> <td>   -3.080</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>48/49</th>           <td>   -3.6064</td> <td>    0.266</td> <td>  -13.567</td> <td> 0.000</td> <td>   -4.127</td> <td>   -3.085</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>49/50</th>           <td>   -3.6091</td> <td>    0.266</td> <td>  -13.576</td> <td> 0.000</td> <td>   -4.130</td> <td>   -3.088</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>50/51</th>           <td>   -3.6087</td> <td>    0.266</td> <td>  -13.575</td> <td> 0.000</td> <td>   -4.130</td> <td>   -3.088</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>51/52</th>           <td>   -3.6107</td> <td>    0.266</td> <td>  -13.582</td> <td> 0.000</td> <td>   -4.132</td> <td>   -3.090</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>52/53</th>           <td>   -3.6131</td> <td>    0.266</td> <td>  -13.591</td> <td> 0.000</td> <td>   -4.134</td> <td>   -3.092</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>53/54</th>           <td>   -3.6148</td> <td>    0.266</td> <td>  -13.597</td> <td> 0.000</td> <td>   -4.136</td> <td>   -3.094</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>54/55</th>           <td>   -3.6149</td> <td>    0.266</td> <td>  -13.598</td> <td> 0.000</td> <td>   -4.136</td> <td>   -3.094</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>55/56</th>           <td>   -3.6140</td> <td>    0.266</td> <td>  -13.594</td> <td> 0.000</td> <td>   -4.135</td> <td>   -3.093</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>56/57</th>           <td>   -3.6124</td> <td>    0.266</td> <td>  -13.588</td> <td> 0.000</td> <td>   -4.133</td> <td>   -3.091</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>57/58</th>           <td>   -3.6087</td> <td>    0.266</td> <td>  -13.574</td> <td> 0.000</td> <td>   -4.130</td> <td>   -3.088</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>58/59</th>           <td>   -3.6032</td> <td>    0.266</td> <td>  -13.554</td> <td> 0.000</td> <td>   -4.124</td> <td>   -3.082</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>59/60</th>           <td>   -3.5969</td> <td>    0.266</td> <td>  -13.531</td> <td> 0.000</td> <td>   -4.118</td> <td>   -3.076</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>60/61</th>           <td>   -3.5896</td> <td>    0.266</td> <td>  -13.504</td> <td> 0.000</td> <td>   -4.111</td> <td>   -3.069</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>61/62</th>           <td>   -3.5816</td> <td>    0.266</td> <td>  -13.474</td> <td> 0.000</td> <td>   -4.103</td> <td>   -3.061</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>62/63</th>           <td>   -3.5731</td> <td>    0.266</td> <td>  -13.443</td> <td> 0.000</td> <td>   -4.094</td> <td>   -3.052</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>63/64</th>           <td>   -3.5643</td> <td>    0.266</td> <td>  -13.410</td> <td> 0.000</td> <td>   -4.085</td> <td>   -3.043</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>64/65</th>           <td>   -3.5569</td> <td>    0.266</td> <td>  -13.383</td> <td> 0.000</td> <td>   -4.078</td> <td>   -3.036</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>65/66</th>           <td>   -3.5471</td> <td>    0.266</td> <td>  -13.347</td> <td> 0.000</td> <td>   -4.068</td> <td>   -3.026</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>66/67</th>           <td>   -3.5352</td> <td>    0.266</td> <td>  -13.303</td> <td> 0.000</td> <td>   -4.056</td> <td>   -3.014</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>67/68</th>           <td>   -3.5252</td> <td>    0.266</td> <td>  -13.266</td> <td> 0.000</td> <td>   -4.046</td> <td>   -3.004</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>68/69</th>           <td>   -3.5136</td> <td>    0.266</td> <td>  -13.223</td> <td> 0.000</td> <td>   -4.034</td> <td>   -2.993</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>69/70</th>           <td>   -3.5010</td> <td>    0.266</td> <td>  -13.176</td> <td> 0.000</td> <td>   -4.022</td> <td>   -2.980</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>70/71</th>           <td>   -3.4858</td> <td>    0.266</td> <td>  -13.120</td> <td> 0.000</td> <td>   -4.007</td> <td>   -2.965</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>71/72</th>           <td>   -3.4664</td> <td>    0.266</td> <td>  -13.048</td> <td> 0.000</td> <td>   -3.987</td> <td>   -2.946</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>72/73</th>           <td>   -3.4462</td> <td>    0.266</td> <td>  -12.974</td> <td> 0.000</td> <td>   -3.967</td> <td>   -2.926</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>73/74</th>           <td>   -3.4269</td> <td>    0.266</td> <td>  -12.902</td> <td> 0.000</td> <td>   -3.948</td> <td>   -2.906</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>74/75</th>           <td>   -3.4083</td> <td>    0.266</td> <td>  -12.833</td> <td> 0.000</td> <td>   -3.929</td> <td>   -2.888</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>75/76</th>           <td>   -3.3895</td> <td>    0.266</td> <td>  -12.764</td> <td> 0.000</td> <td>   -3.910</td> <td>   -2.869</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>76/77</th>           <td>   -3.3674</td> <td>    0.266</td> <td>  -12.682</td> <td> 0.000</td> <td>   -3.888</td> <td>   -2.847</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>77/78</th>           <td>   -3.3423</td> <td>    0.265</td> <td>  -12.589</td> <td> 0.000</td> <td>   -3.863</td> <td>   -2.822</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>78/79</th>           <td>   -3.3185</td> <td>    0.265</td> <td>  -12.501</td> <td> 0.000</td> <td>   -3.839</td> <td>   -2.798</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>79/80</th>           <td>   -3.2908</td> <td>    0.265</td> <td>  -12.399</td> <td> 0.000</td> <td>   -3.811</td> <td>   -2.771</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>80/81</th>           <td>   -3.2592</td> <td>    0.265</td> <td>  -12.282</td> <td> 0.000</td> <td>   -3.779</td> <td>   -2.739</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>81/82</th>           <td>   -3.2287</td> <td>    0.265</td> <td>  -12.169</td> <td> 0.000</td> <td>   -3.749</td> <td>   -2.709</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>82/83</th>           <td>   -3.1990</td> <td>    0.265</td> <td>  -12.060</td> <td> 0.000</td> <td>   -3.719</td> <td>   -2.679</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>83/84</th>           <td>   -3.1640</td> <td>    0.265</td> <td>  -11.930</td> <td> 0.000</td> <td>   -3.684</td> <td>   -2.644</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>84/85</th>           <td>   -3.1219</td> <td>    0.265</td> <td>  -11.775</td> <td> 0.000</td> <td>   -3.642</td> <td>   -2.602</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>85/86</th>           <td>   -3.0835</td> <td>    0.265</td> <td>  -11.634</td> <td> 0.000</td> <td>   -3.603</td> <td>   -2.564</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>86/87</th>           <td>   -3.0442</td> <td>    0.265</td> <td>  -11.488</td> <td> 0.000</td> <td>   -3.564</td> <td>   -2.525</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>87/88</th>           <td>   -2.9948</td> <td>    0.265</td> <td>  -11.306</td> <td> 0.000</td> <td>   -3.514</td> <td>   -2.476</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>88/89</th>           <td>   -2.9406</td> <td>    0.265</td> <td>  -11.106</td> <td> 0.000</td> <td>   -3.460</td> <td>   -2.422</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>89/90</th>           <td>   -2.8827</td> <td>    0.265</td> <td>  -10.892</td> <td> 0.000</td> <td>   -3.401</td> <td>   -2.364</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>90/91</th>           <td>   -2.8194</td> <td>    0.265</td> <td>  -10.659</td> <td> 0.000</td> <td>   -3.338</td> <td>   -2.301</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>91/92</th>           <td>   -2.7536</td> <td>    0.264</td> <td>  -10.416</td> <td> 0.000</td> <td>   -3.272</td> <td>   -2.235</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>92/93</th>           <td>   -2.6749</td> <td>    0.264</td> <td>  -10.125</td> <td> 0.000</td> <td>   -3.193</td> <td>   -2.157</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>93/94</th>           <td>   -2.5761</td> <td>    0.264</td> <td>   -9.760</td> <td> 0.000</td> <td>   -3.093</td> <td>   -2.059</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>94/95</th>           <td>   -2.4602</td> <td>    0.264</td> <td>   -9.333</td> <td> 0.000</td> <td>   -2.977</td> <td>   -1.944</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>95/96</th>           <td>   -2.3271</td> <td>    0.263</td> <td>   -8.842</td> <td> 0.000</td> <td>   -2.843</td> <td>   -1.811</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>96/97</th>           <td>   -2.1707</td> <td>    0.263</td> <td>   -8.263</td> <td> 0.000</td> <td>   -2.686</td> <td>   -1.656</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>97/98</th>           <td>   -1.9721</td> <td>    0.262</td> <td>   -7.526</td> <td> 0.000</td> <td>   -2.486</td> <td>   -1.459</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>98/99</th>           <td>   -1.6922</td> <td>    0.261</td> <td>   -6.478</td> <td> 0.000</td> <td>   -2.204</td> <td>   -1.180</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>99/100</th>          <td>   -1.2341</td> <td>    0.261</td> <td>   -4.734</td> <td> 0.000</td> <td>   -1.745</td> <td>   -0.723</td>\n",
              "</tr>\n",
              "</table>"
            ],
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &         y          & \\textbf{  Log-Likelihood:    } &   -6220.9   \\\\\n\\textbf{Model:}            &    OrderedModel    & \\textbf{  AIC:               } & 1.266e+04   \\\\\n\\textbf{Method:}           & Maximum Likelihood & \\textbf{  BIC:               } & 1.322e+04   \\\\\n\\textbf{Date:}             &  Thu, 26 Oct 2023  & \\textbf{                     } &             \\\\\n\\textbf{Time:}             &      17:34:35      & \\textbf{                     } &             \\\\\n\\textbf{No. Observations:} &         1372       & \\textbf{                     } &             \\\\\n\\textbf{Df Residuals:}     &         1264       & \\textbf{                     } &             \\\\\n\\textbf{Df Model:}         &           11       & \\textbf{                     } &             \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                         & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{TitleFlag}       &       0.6330  &        0.223     &     2.842  &         0.004        &        0.196    &        1.070     \\\\\n\\textbf{TitleDensity}    &       0.5903  &        0.204     &     2.895  &         0.004        &        0.191    &        0.990     \\\\\n\\textbf{URLFlag}         &      -0.0890  &        0.234     &    -0.381  &         0.703        &       -0.547    &        0.369     \\\\\n\\textbf{URLDensity}      &       1.3060  &        0.269     &     4.854  &         0.000        &        0.779    &        1.833     \\\\\n\\textbf{MetaFlag}        &      -0.1294  &        0.075     &    -1.730  &         0.084        &       -0.276    &        0.017     \\\\\n\\textbf{MetaDensity}     &       0.8170  &        0.423     &     1.929  &         0.054        &       -0.013    &        1.647     \\\\\n\\textbf{PageAuthority}   &       0.0016  &        0.012     &     0.137  &         0.891        &       -0.022    &        0.025     \\\\\n\\textbf{DomainAuthority} &       0.0037  &        0.005     &     0.791  &         0.429        &       -0.005    &        0.013     \\\\\n\\textbf{LinkingDomain}   &      -0.0133  &        0.061     &    -0.218  &         0.827        &       -0.133    &        0.106     \\\\\n\\textbf{InboundLink}     &      -0.0191  &        0.031     &    -0.622  &         0.534        &       -0.079    &        0.041     \\\\\n\\textbf{RankingKeyword}  &       0.0414  &        0.020     &     2.028  &         0.043        &        0.001    &        0.081     \\\\\n\\textbf{3/4}             &      -1.7461  &        0.161     &   -10.845  &         0.000        &       -2.062    &       -1.431     \\\\\n\\textbf{4/5}             &      -1.2722  &        0.261     &    -4.872  &         0.000        &       -1.784    &       -0.760     \\\\\n\\textbf{5/6}             &      -1.7326  &        0.262     &    -6.622  &         0.000        &       -2.245    &       -1.220     \\\\\n\\textbf{6/7}             &      -2.0233  &        0.262     &    -7.709  &         0.000        &       -2.538    &       -1.509     \\\\\n\\textbf{7/8}             &      -2.2335  &        0.263     &    -8.490  &         0.000        &       -2.749    &       -1.718     \\\\\n\\textbf{8/9}             &      -2.3962  &        0.264     &    -9.093  &         0.000        &       -2.913    &       -1.880     \\\\\n\\textbf{9/10}            &      -2.5229  &        0.264     &    -9.563  &         0.000        &       -3.040    &       -2.006     \\\\\n\\textbf{10/11}           &      -2.6232  &        0.264     &    -9.935  &         0.000        &       -3.141    &       -2.106     \\\\\n\\textbf{11/12}           &      -2.7155  &        0.264     &   -10.275  &         0.000        &       -3.233    &       -2.198     \\\\\n\\textbf{12/13}           &      -2.8004  &        0.264     &   -10.589  &         0.000        &       -3.319    &       -2.282     \\\\\n\\textbf{13/14}           &      -2.8741  &        0.265     &   -10.860  &         0.000        &       -3.393    &       -2.355     \\\\\n\\textbf{14/15}           &      -2.9390  &        0.265     &   -11.100  &         0.000        &       -3.458    &       -2.420     \\\\\n\\textbf{15/16}           &      -2.9934  &        0.265     &   -11.301  &         0.000        &       -3.513    &       -2.474     \\\\\n\\textbf{16/17}           &      -3.0443  &        0.265     &   -11.489  &         0.000        &       -3.564    &       -2.525     \\\\\n\\textbf{17/18}           &      -3.0910  &        0.265     &   -11.662  &         0.000        &       -3.610    &       -2.571     \\\\\n\\textbf{18/19}           &      -3.1337  &        0.265     &   -11.819  &         0.000        &       -3.653    &       -2.614     \\\\\n\\textbf{19/20}           &      -3.1773  &        0.265     &   -11.980  &         0.000        &       -3.697    &       -2.657     \\\\\n\\textbf{20/21}           &      -3.2160  &        0.265     &   -12.123  &         0.000        &       -3.736    &       -2.696     \\\\\n\\textbf{21/22}           &      -3.2487  &        0.265     &   -12.244  &         0.000        &       -3.769    &       -2.729     \\\\\n\\textbf{22/23}           &      -3.2792  &        0.265     &   -12.357  &         0.000        &       -3.799    &       -2.759     \\\\\n\\textbf{23/24}           &      -3.3089  &        0.265     &   -12.466  &         0.000        &       -3.829    &       -2.789     \\\\\n\\textbf{24/25}           &      -3.3370  &        0.265     &   -12.570  &         0.000        &       -3.857    &       -2.817     \\\\\n\\textbf{25/26}           &      -3.3610  &        0.266     &   -12.659  &         0.000        &       -3.881    &       -2.841     \\\\\n\\textbf{26/27}           &      -3.3824  &        0.266     &   -12.738  &         0.000        &       -3.903    &       -2.862     \\\\\n\\textbf{27/28}           &      -3.4034  &        0.266     &   -12.816  &         0.000        &       -3.924    &       -2.883     \\\\\n\\textbf{28/29}           &      -3.4225  &        0.266     &   -12.886  &         0.000        &       -3.943    &       -2.902     \\\\\n\\textbf{29/30}           &      -3.4382  &        0.266     &   -12.945  &         0.000        &       -3.959    &       -2.918     \\\\\n\\textbf{30/31}           &      -3.4532  &        0.266     &   -13.000  &         0.000        &       -3.974    &       -2.933     \\\\\n\\textbf{31/32}           &      -3.4707  &        0.266     &   -13.065  &         0.000        &       -3.991    &       -2.950     \\\\\n\\textbf{32/33}           &      -3.4870  &        0.266     &   -13.125  &         0.000        &       -4.008    &       -2.966     \\\\\n\\textbf{33/34}           &      -3.4993  &        0.266     &   -13.171  &         0.000        &       -4.020    &       -2.979     \\\\\n\\textbf{34/35}           &      -3.5110  &        0.266     &   -13.214  &         0.000        &       -4.032    &       -2.990     \\\\\n\\textbf{35/36}           &      -3.5221  &        0.266     &   -13.255  &         0.000        &       -4.043    &       -3.001     \\\\\n\\textbf{36/37}           &      -3.5320  &        0.266     &   -13.291  &         0.000        &       -4.053    &       -3.011     \\\\\n\\textbf{37/38}           &      -3.5415  &        0.266     &   -13.327  &         0.000        &       -4.062    &       -3.021     \\\\\n\\textbf{38/39}           &      -3.5526  &        0.266     &   -13.368  &         0.000        &       -4.073    &       -3.032     \\\\\n\\textbf{39/40}           &      -3.5623  &        0.266     &   -13.404  &         0.000        &       -4.083    &       -3.041     \\\\\n\\textbf{40/41}           &      -3.5696  &        0.266     &   -13.431  &         0.000        &       -4.091    &       -3.049     \\\\\n\\textbf{41/42}           &      -3.5770  &        0.266     &   -13.458  &         0.000        &       -4.098    &       -3.056     \\\\\n\\textbf{42/43}           &      -3.5842  &        0.266     &   -13.485  &         0.000        &       -4.105    &       -3.063     \\\\\n\\textbf{43/44}           &      -3.5910  &        0.266     &   -13.510  &         0.000        &       -4.112    &       -3.070     \\\\\n\\textbf{44/45}           &      -3.5941  &        0.266     &   -13.521  &         0.000        &       -4.115    &       -3.073     \\\\\n\\textbf{45/46}           &      -3.5959  &        0.266     &   -13.528  &         0.000        &       -4.117    &       -3.075     \\\\\n\\textbf{46/47}           &      -3.5978  &        0.266     &   -13.534  &         0.000        &       -4.119    &       -3.077     \\\\\n\\textbf{47/48}           &      -3.6007  &        0.266     &   -13.545  &         0.000        &       -4.122    &       -3.080     \\\\\n\\textbf{48/49}           &      -3.6064  &        0.266     &   -13.567  &         0.000        &       -4.127    &       -3.085     \\\\\n\\textbf{49/50}           &      -3.6091  &        0.266     &   -13.576  &         0.000        &       -4.130    &       -3.088     \\\\\n\\textbf{50/51}           &      -3.6087  &        0.266     &   -13.575  &         0.000        &       -4.130    &       -3.088     \\\\\n\\textbf{51/52}           &      -3.6107  &        0.266     &   -13.582  &         0.000        &       -4.132    &       -3.090     \\\\\n\\textbf{52/53}           &      -3.6131  &        0.266     &   -13.591  &         0.000        &       -4.134    &       -3.092     \\\\\n\\textbf{53/54}           &      -3.6148  &        0.266     &   -13.597  &         0.000        &       -4.136    &       -3.094     \\\\\n\\textbf{54/55}           &      -3.6149  &        0.266     &   -13.598  &         0.000        &       -4.136    &       -3.094     \\\\\n\\textbf{55/56}           &      -3.6140  &        0.266     &   -13.594  &         0.000        &       -4.135    &       -3.093     \\\\\n\\textbf{56/57}           &      -3.6124  &        0.266     &   -13.588  &         0.000        &       -4.133    &       -3.091     \\\\\n\\textbf{57/58}           &      -3.6087  &        0.266     &   -13.574  &         0.000        &       -4.130    &       -3.088     \\\\\n\\textbf{58/59}           &      -3.6032  &        0.266     &   -13.554  &         0.000        &       -4.124    &       -3.082     \\\\\n\\textbf{59/60}           &      -3.5969  &        0.266     &   -13.531  &         0.000        &       -4.118    &       -3.076     \\\\\n\\textbf{60/61}           &      -3.5896  &        0.266     &   -13.504  &         0.000        &       -4.111    &       -3.069     \\\\\n\\textbf{61/62}           &      -3.5816  &        0.266     &   -13.474  &         0.000        &       -4.103    &       -3.061     \\\\\n\\textbf{62/63}           &      -3.5731  &        0.266     &   -13.443  &         0.000        &       -4.094    &       -3.052     \\\\\n\\textbf{63/64}           &      -3.5643  &        0.266     &   -13.410  &         0.000        &       -4.085    &       -3.043     \\\\\n\\textbf{64/65}           &      -3.5569  &        0.266     &   -13.383  &         0.000        &       -4.078    &       -3.036     \\\\\n\\textbf{65/66}           &      -3.5471  &        0.266     &   -13.347  &         0.000        &       -4.068    &       -3.026     \\\\\n\\textbf{66/67}           &      -3.5352  &        0.266     &   -13.303  &         0.000        &       -4.056    &       -3.014     \\\\\n\\textbf{67/68}           &      -3.5252  &        0.266     &   -13.266  &         0.000        &       -4.046    &       -3.004     \\\\\n\\textbf{68/69}           &      -3.5136  &        0.266     &   -13.223  &         0.000        &       -4.034    &       -2.993     \\\\\n\\textbf{69/70}           &      -3.5010  &        0.266     &   -13.176  &         0.000        &       -4.022    &       -2.980     \\\\\n\\textbf{70/71}           &      -3.4858  &        0.266     &   -13.120  &         0.000        &       -4.007    &       -2.965     \\\\\n\\textbf{71/72}           &      -3.4664  &        0.266     &   -13.048  &         0.000        &       -3.987    &       -2.946     \\\\\n\\textbf{72/73}           &      -3.4462  &        0.266     &   -12.974  &         0.000        &       -3.967    &       -2.926     \\\\\n\\textbf{73/74}           &      -3.4269  &        0.266     &   -12.902  &         0.000        &       -3.948    &       -2.906     \\\\\n\\textbf{74/75}           &      -3.4083  &        0.266     &   -12.833  &         0.000        &       -3.929    &       -2.888     \\\\\n\\textbf{75/76}           &      -3.3895  &        0.266     &   -12.764  &         0.000        &       -3.910    &       -2.869     \\\\\n\\textbf{76/77}           &      -3.3674  &        0.266     &   -12.682  &         0.000        &       -3.888    &       -2.847     \\\\\n\\textbf{77/78}           &      -3.3423  &        0.265     &   -12.589  &         0.000        &       -3.863    &       -2.822     \\\\\n\\textbf{78/79}           &      -3.3185  &        0.265     &   -12.501  &         0.000        &       -3.839    &       -2.798     \\\\\n\\textbf{79/80}           &      -3.2908  &        0.265     &   -12.399  &         0.000        &       -3.811    &       -2.771     \\\\\n\\textbf{80/81}           &      -3.2592  &        0.265     &   -12.282  &         0.000        &       -3.779    &       -2.739     \\\\\n\\textbf{81/82}           &      -3.2287  &        0.265     &   -12.169  &         0.000        &       -3.749    &       -2.709     \\\\\n\\textbf{82/83}           &      -3.1990  &        0.265     &   -12.060  &         0.000        &       -3.719    &       -2.679     \\\\\n\\textbf{83/84}           &      -3.1640  &        0.265     &   -11.930  &         0.000        &       -3.684    &       -2.644     \\\\\n\\textbf{84/85}           &      -3.1219  &        0.265     &   -11.775  &         0.000        &       -3.642    &       -2.602     \\\\\n\\textbf{85/86}           &      -3.0835  &        0.265     &   -11.634  &         0.000        &       -3.603    &       -2.564     \\\\\n\\textbf{86/87}           &      -3.0442  &        0.265     &   -11.488  &         0.000        &       -3.564    &       -2.525     \\\\\n\\textbf{87/88}           &      -2.9948  &        0.265     &   -11.306  &         0.000        &       -3.514    &       -2.476     \\\\\n\\textbf{88/89}           &      -2.9406  &        0.265     &   -11.106  &         0.000        &       -3.460    &       -2.422     \\\\\n\\textbf{89/90}           &      -2.8827  &        0.265     &   -10.892  &         0.000        &       -3.401    &       -2.364     \\\\\n\\textbf{90/91}           &      -2.8194  &        0.265     &   -10.659  &         0.000        &       -3.338    &       -2.301     \\\\\n\\textbf{91/92}           &      -2.7536  &        0.264     &   -10.416  &         0.000        &       -3.272    &       -2.235     \\\\\n\\textbf{92/93}           &      -2.6749  &        0.264     &   -10.125  &         0.000        &       -3.193    &       -2.157     \\\\\n\\textbf{93/94}           &      -2.5761  &        0.264     &    -9.760  &         0.000        &       -3.093    &       -2.059     \\\\\n\\textbf{94/95}           &      -2.4602  &        0.264     &    -9.333  &         0.000        &       -2.977    &       -1.944     \\\\\n\\textbf{95/96}           &      -2.3271  &        0.263     &    -8.842  &         0.000        &       -2.843    &       -1.811     \\\\\n\\textbf{96/97}           &      -2.1707  &        0.263     &    -8.263  &         0.000        &       -2.686    &       -1.656     \\\\\n\\textbf{97/98}           &      -1.9721  &        0.262     &    -7.526  &         0.000        &       -2.486    &       -1.459     \\\\\n\\textbf{98/99}           &      -1.6922  &        0.261     &    -6.478  &         0.000        &       -2.204    &       -1.180     \\\\\n\\textbf{99/100}          &      -1.2341  &        0.261     &    -4.734  &         0.000        &       -1.745    &       -0.723     \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{OrderedModel Results}\n\\end{center}"
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "# Print model summary by model.summary()\n",
        "results.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqaxOS0SHh6K",
        "outputId": "92a6266f-2a89-4c39-dce1-607ce5d53b32"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "FrntqUiMHq8m"
      },
      "outputs": [],
      "source": [
        "# Make prediction: assume the predicted output of this model is y_pred,\n",
        "# for this question, you need to do y_pred.argmax(1)+3 to get the real output.\n",
        "# Please refer to the reference above for how to make prediction properly.\n",
        "y_pred_probabilities = results.predict(X_test)\n",
        "predictions = y_pred_probabilities.to_numpy().argmax(1) + 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "zKzlhwKHHq8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7785923-fb8d-4655-ca50-a7645c5c4edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 53.18070209041934\n"
          ]
        }
      ],
      "source": [
        "# Print RMSE, y_test is a numpy array from test_label, y_pred is a numpy array from your model prediction\n",
        "resids = y_test - predictions\n",
        "mse = np.mean(resids**2)\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"RMSE:\", rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTJliTGCHq8m"
      },
      "source": [
        "Pick the most statistically significant variable and interpret its estimated beta coefficient. (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHgJ3JnCHq8m"
      },
      "source": [
        "*ANSWER: HERE*\n",
        "Both URLDensity and DomainAuthority are statistically significant, both with P-values close to 0. DomainAuthority has such low P-values in this case but its 95%CI is closer 0. For a 1 unit increase in DomainAuthority, it gives a positive change in dependent variable ReverseRank by 0.0133, holding all other variables constant.\n",
        "\n",
        "URLDensity 0.775\t1.825\n",
        "\n",
        "DomainAuthority\t0.007\t0.019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cv7_qbxHq8m"
      },
      "source": [
        "### Q3. Decision Tree (5 points)\n",
        "Please use the training dataset to fit a decision tree model with all variables aforementioned. **Use the `ReverseRank` as the target variable**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "2HCf7ehjHq8m"
      },
      "outputs": [],
      "source": [
        "# Data Input\n",
        "X_train = train_feature # trainig feature, as a dataframe in pandas\n",
        "X_test = test_feature # test feature, as a df\n",
        "\n",
        "y_train = train_label.values # training label, as a numpy array\n",
        "y_test = test_label.values # test label, as a np array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "mS3z1RbMHq8n"
      },
      "outputs": [],
      "source": [
        "# Model & Fit: USE DecisionTreeClassifier(random_state=0).fit(...)\n",
        "# y_train is a numpy array from train_label, X_train is a pandas dataframe from train_feature.\n",
        "# Make prediction\n",
        "results = DecisionTreeRegressor(random_state=0)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "results.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = results.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "el0up4KsHq8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16a5cbc-4c7f-4ec9-8764-51c2bacbbe3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 42.57505876979738\n"
          ]
        }
      ],
      "source": [
        "# Print RMSE, y_test is a numpy array from test_label, y_pred is a numpy array from your model prediction\n",
        "resids = y_test - predictions\n",
        "mse = np.mean(resids**2)\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"RMSE:\", rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc9AEHjUHq8n"
      },
      "source": [
        "###  Q4. Random Forest (5 points)\n",
        "Please use the training dataset to fit a random forest model with all variables aforementioned. **Use the `ReverseRank` as the target variable**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "B_6_zUjOHq8n"
      },
      "outputs": [],
      "source": [
        "# Data Input\n",
        "X_train = train_feature # trainig feature, as a dataframe in pandas\n",
        "X_test = test_feature # test feature, as a df\n",
        "\n",
        "y_train = train_label.values # training label, as a numpy array\n",
        "y_test = test_label.values # test label, as a np array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "U1tAsBeUHq8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c3d861-84a0-4a3e-af0d-8a67b7b2d49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-76-791b7e75c08f>:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  results.fit(X_train, y_train)\n"
          ]
        }
      ],
      "source": [
        "# Model & Fit: USE RandomForestRegressor(random_state=0).fit(...)\n",
        "# y_train is a numpy array from train_label, X_train is a pandas dataframe from train_feature.\n",
        "# Make prediction\n",
        "results = RandomForestRegressor(random_state=0)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "results.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = results.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "IKK6f2LDHq8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee95ab0-ff28-4d8f-c08b-bb967fcb3288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 31.89853003853016\n"
          ]
        }
      ],
      "source": [
        "# Print RMSE, y_test is a numpy array from test_label, y_pred is a numpy array from your model prediction\n",
        "resids = y_test - predictions\n",
        "mse = np.mean(resids**2)\n",
        "\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(\"RMSE:\", rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upVI5rueHq8w"
      },
      "source": [
        "## Problem 2. Pairwise Rank with `xgboost` (35 points)\n",
        "Please use the training dataset to fit an XGboost model with all variables aforementioned. **Use the `ReverseRank` as the target variable**.\n",
        "\n",
        "Show the feature importance plot. Use \"Gain\" for `measure` in the importance plot.\n",
        "\n",
        "For the XGboost prediction, please interpret the importance value of the most important variable. Please use the trained XGboost model to predict the rank in the test dataset and report the RMSE of the prediction.\n",
        "\n",
        "Reference: https://xgboost.readthedocs.io/en/latest/python/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUhecNNFHq8w"
      },
      "outputs": [],
      "source": [
        "# Data Input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22HA_04fHq8w"
      },
      "source": [
        "### Q1. Use `reg:linear` for `objective` and `rmse` for `eval_metric`. (10 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5tgSwqpHq8w"
      },
      "outputs": [],
      "source": [
        "# Model and Parameter Setting\n",
        "# Use xgb.XGBRegressor\n",
        "# Use parameters below first and you can adjust it later.\n",
        "reg = xgb.XGBRegressor(max_depth = 6,\n",
        "                       eta = 0.1,\n",
        "                       gamma = 0.1,\n",
        "                       subsample = 0.8,\n",
        "                       colsample_bytree = 0.8,\n",
        "                       alpha = 0.5,\n",
        "                       objective = \"reg:linear\",\n",
        "                       eval_metric = \"rmse\",\n",
        "                       seed = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKYPsvceHq8x"
      },
      "outputs": [],
      "source": [
        "# Fit the model by your_model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU9MO9uhHq8x"
      },
      "outputs": [],
      "source": [
        "# Show the feature importance plot. Use plot_importance(your_model, importance_type = 'gain')\n",
        "# If you don't see your plot, please run this cell again.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kB3_b7hHq8x"
      },
      "source": [
        "**Please interpret the importance value of the most important variable.** (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9riB78IGHq8x"
      },
      "source": [
        "*ANSWER: HERE*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEw6rptNHq8x"
      },
      "outputs": [],
      "source": [
        "# Make prediction: y_pred should be the output of your prediction, it cotains value\n",
        "# from 3 to 100 (may not be integers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFS2QK3AHq8x"
      },
      "outputs": [],
      "source": [
        "## Print RMSE, y_test is a numpy array from test_label, y_pred is a numpy array from your model prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ASbXHcHq8x"
      },
      "source": [
        "**Try different combinations of the hyper-parameters to get a lower RMSE.** (5 points)\n",
        "\n",
        "Please note that you can use any hyper-parameter tuning techniques in the lecture. The points are not awarded based on the exact number of RMSE. As long as it is lower than the original values and you can justify the technique you use, you will be awarded the points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecDmSXKLHq8x"
      },
      "outputs": [],
      "source": [
        "# your code here...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2Cs_1YeHq8y"
      },
      "source": [
        "### Q2. Redo Q1 by using `rank:pairwise` for `objective` and `rmse` for `eval_metric`. (15 points)\n",
        "Observe that we did not utilize the query information yet. However, in the dataset, we know that each query corresponds to 98 data points. Now we will leverage this information by setting group information for both training and testing data. (5 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz7WAsf5Hq8y"
      },
      "outputs": [],
      "source": [
        "# Model and Parameter Setting\n",
        "# Use xgb.XGBRanker\n",
        "# Use parameters below first and you can adjust it later.\n",
        "reg = xgb.XGBRanker(max_depth = 6,\n",
        "                           eta = 0.1,\n",
        "                           gamma = 0.1,\n",
        "                           subsample = 0.8,\n",
        "                           colsample_bytree = 0.8,\n",
        "                           alpha = 0.5,\n",
        "                           objective = \"rank:pairwise\",\n",
        "                           eval_metric = \"rmse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDDqPqM-Hq8y"
      },
      "outputs": [],
      "source": [
        "# Data Input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq6IN6tgHq8y"
      },
      "outputs": [],
      "source": [
        "# Fit the model by your_model.fit(X_train, y_train, group = np.full(14,98))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCSiUM9lHq8y"
      },
      "outputs": [],
      "source": [
        "# Show the feature importance plot. Use plot_importance(your_model, importance_type = 'gain')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYbG5fC7Hq8z"
      },
      "source": [
        "**Please interpret the importance value of the most important variable.** (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLBve2EoHq8z"
      },
      "source": [
        "*ANSWER: HERE*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3zDZJ89Hq8z"
      },
      "source": [
        "Note that here `y_pred` is *not* a direct rank on itself. We need to figure out how to get the rank from `y_pred`. Test data can be divided into 6 groups, we need to perform the following steps for each group.\n",
        "1. For each group, get `y_pred` from your model.\n",
        "2. The values in `y_pred` indicate a relative score for the rank. The higher the score, the better the page rank. For all 98 pages in one group, the page with the highest score should rank as 100, and the page with the lowest score should rank as 3.\n",
        "3. Repeat this for all groups. You are required to implement this by some simple codes. **y_pred_rank** is the predicted ranks you transformed from **y_pred**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvjsyni3Hq8z"
      },
      "outputs": [],
      "source": [
        "# Make prediction: y_pred is the predicted output from your model, y_pred_rank is the actual rank you get\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF3547X4Hq8z"
      },
      "outputs": [],
      "source": [
        "# Print RMSE, y_test is a numpy array from test_label, y_pred_rank is a numpy array from your model prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi0hrXuHHq8z"
      },
      "source": [
        "**Try different combinations of the hyper-parameters to get a lower RMSE.** (5 points)\n",
        "\n",
        "Please note that you can use any hyper-parameter tuning techniques in the lecture. The points are not awarded based on the exact number of RMSE. As long as it is lower than the original values and you can justify the technique you use, you will be awarded the points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpwOaykMHq8z"
      },
      "outputs": [],
      "source": [
        "# your code here...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5XgM63YHq8z"
      },
      "source": [
        "### Q3. Use `rank:ndcg` for `objective` and `rmse` for `eval_metric`. Redo Question 6. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq6fvhdfHq80"
      },
      "outputs": [],
      "source": [
        "# Model and Parameter Setting\n",
        "# Use xgb.XGBRanker\n",
        "# Use parameters below first and you can adjust it later.\n",
        "reg = xgb.XGBRanker(max_depth = 6,\n",
        "                           eta = 0.1,\n",
        "                           gamma = 0.1,\n",
        "                           subsample = 0.8,\n",
        "                           colsample_bytree = 0.8,\n",
        "                           alpha = 0.5,\n",
        "                           objective = \"rank:ndcg\",\n",
        "                           eval_metric = \"rmse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7XSwi1dHq80"
      },
      "outputs": [],
      "source": [
        "# Data Input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBoClXL_Hq80"
      },
      "outputs": [],
      "source": [
        "# Fit the model by your_model.fit(X_train, y_train, group = np.full(14,98))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--v8qib2Hq80"
      },
      "outputs": [],
      "source": [
        "# Show the feature importance plot. Use plot_importance(your_model, importance_type = 'gain')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RixxGnEZHq80"
      },
      "source": [
        "**Please interpret the importance value of the most important variable.** (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bKzUMQqHq80"
      },
      "source": [
        "*ANSWER: HERE*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHDQG8YmHq80"
      },
      "outputs": [],
      "source": [
        "# Make prediction: y_pred is the predicted output from your model, y_pred_rank is the actual rank you get\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KN2K-rFEHq81"
      },
      "outputs": [],
      "source": [
        "# Print RMSE, y_test is a numpy array from test_label, y_pred_rank is a numpy array from your model prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0loF3HEHq81"
      },
      "source": [
        "**Try different combinations of the hyper-parameters to get a lower RMSE.** (5 points)\n",
        "\n",
        "Please note that you can use any hyper-parameter tuning techniques in the lecture. The points are not awarded based on the exact number of RMSE. As long as it is lower than the original values and you can justify the technique you use, you will be awarded the points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuwE8obIHq81"
      },
      "outputs": [],
      "source": [
        "# your code here...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh3wSPJPHq81"
      },
      "source": [
        "## Problem 3. Interpretation Short-Answers (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihbFL7O0Hq81"
      },
      "source": [
        "Please paste the RMSE values of all questions above in the table below. Please round your RMSE up to 4 decimals.\n",
        "\n",
        "To access the table written in markdown below, double click the placeholder table, edit the corresponding value and run the cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GVJyH9oHq81"
      },
      "source": [
        "Before Tuning:\n",
        "\n",
        "| Question | P1-Q1 | P1-Q2 | P1-Q3 | P1-Q4 | P2-Q5 | P2-Q6 | P2-Q7 |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| RMSE | 1.0000 | 2.0000 | 3.0000 | 4.0000 | 5.0000 | 6.0000 | 7.0000 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn1YLcysHq81"
      },
      "source": [
        "After Tuning:\n",
        "\n",
        "| Question | P2-Q5 | P2-Q6 | P2-Q7 |\n",
        "| --- | --- | --- | --- |\n",
        "| RMSE |  5.0000 | 6.0000 | 7.0000 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hTceTQAHq81"
      },
      "source": [
        "**Question: Please describe what you observe from the RMSE value for all methods of pointwise ranking. What is the best method? Why? (5 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR51Ml_YHq82"
      },
      "source": [
        "Answer: HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIs_CG6aHq82"
      },
      "source": [
        "**Question: Please compare the results from pointwise ranking and pairwise ranking and describe your observatins.(5 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnl4_MNQHq82"
      },
      "source": [
        "Answer: HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIPZkObLHq82"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "1. ID: identification number (i.e., row number in the dataset)\n",
        "\n",
        "2. Position: the actual google ranking of the webpage to the query\n",
        "\n",
        "3. ReverseRank: equals $101$ minus Position. It is equivalent to the position. Sometimes using ReverseRank as the dependent variable can have a better prediction.\n",
        "\n",
        "4. Title: the title of the webpage\n",
        "\n",
        "5. URL: the URL of the webpage\n",
        "\n",
        "6. Meta: the meta description of the webpage\n",
        "\n",
        "7.\tTitleFlag: indicates that whether the whole keyword is included in the page title. TitleFlag equals 1 if yes, otherwise, 0.\n",
        "\n",
        "8.\tUrlFlag: indicates that whether the whole keyword is included in the page url. UrlFlag equals 1 if yes, otherwise, 0.\n",
        "\n",
        "9.\tMetaFlag: indicates that whether the whole keyword is included in the page meta description. MetaFlag equals 1 if yes, otherwise, 0.\n",
        "\n",
        "10.\tTitleDensity: is the percentage of times a keyword appears in the title of a web page compared to the total number of words in the title of a web page.\n",
        "\n",
        "11.\tUrlDensity: is the percentage of times a keyword appears in the URL of a web page compared to the total number of words in the URL of a web page.\n",
        "\n",
        "12.\tMetaDensity: is the percentage of times a keyword appears in the meta description of a web page compared to the total number of words in the meta description of a web page.\n",
        "\n",
        "13.\tPageAuthority: is a score developed by Moz that predicts how well a specific page will rank on search engine result pages (SERP). https://moz.com/learn/seo/page-authority\n",
        "\n",
        "14.\tDomainAuthority: is a search engine ranking score developed by Moz that predicts how well a website will rank on search engine result pages (SERPs). https://moz.com/learn/seo/domain-authority\n",
        "\n",
        "15.\tLinkingDomain: is the number of unique external domains linking to this page. Two or more links from the same websites are considered as one linking domain. Provided by Moz.\n",
        "\n",
        "16.\tInboundLink: is the number of unique external pages linking to this page. Two or more links from the same page on a website are considered as one inbound link. Provided by Moz.\n",
        "\n",
        "17.\tRankingKeyword: is the number of keywords for which this site ranks within the top 50 positions on Google US. Provided by Moz."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "273px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}